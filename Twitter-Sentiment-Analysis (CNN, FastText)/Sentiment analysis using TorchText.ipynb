{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data, vocab\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "SEED = 1994\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating bigrams as suggested by FastText. It appends bigrams to the end of the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bigrams(x):\n",
    "    n_grams = set(zip(*[x[i:] for i in range(2)]))\n",
    "    for n_gram in n_grams:\n",
    "        x.append(' '.join(n_gram))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize='spacy', preprocessing=generate_bigrams)\n",
    "LABEL = data.LabelField(sequential=False, use_vocab=False, dtype = torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training only on 10% of the original dataset ~150,000 tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.TabularDataset(\n",
    "        path='subset.csv', format='csv',\n",
    "        fields=[('text', TEXT), ('label', LABEL)],\n",
    "        skip_header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = df.split(random_state=random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 112000\n",
      "Number of validation examples: 48000\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'feel',\n",
       " 'so',\n",
       " 'unbelievably',\n",
       " 'hideously',\n",
       " 'crap',\n",
       " 'today',\n",
       " ' ',\n",
       " 'but',\n",
       " 'i',\n",
       " 'have',\n",
       " 'so',\n",
       " 'much',\n",
       " 'work',\n",
       " 'to',\n",
       " 'do',\n",
       " 'i',\n",
       " 'should',\n",
       " 'just',\n",
       " 'get',\n",
       " 'on',\n",
       " 'with',\n",
       " 'it',\n",
       " '.',\n",
       " 'should just',\n",
       " '  but',\n",
       " 'get on',\n",
       " 'just get',\n",
       " 'so unbelievably',\n",
       " 'feel so',\n",
       " 'to do',\n",
       " 'do i',\n",
       " 'work to',\n",
       " 'i have',\n",
       " 'today  ',\n",
       " 'crap today',\n",
       " 'much work',\n",
       " 'but i',\n",
       " 'hideously crap',\n",
       " 'it .',\n",
       " 'unbelievably hideously',\n",
       " 'on with',\n",
       " 'i feel',\n",
       " 'with it',\n",
       " 'so much',\n",
       " 'have so',\n",
       " 'i should']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[5].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[5].label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use pretrained GloVe vectors. The pretrained word embeddings for Twitter data comes in 4 sizes. I have chosen the largest word embedding (200 dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = vocab.Vectors('glove.twitter.27B.200d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data, valid_data, max_size = 25000, vectors = vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 25002\n",
      "Unique tokens in LABEL vocabulary: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1135,  0.2170,  0.4643,  0.7430, -0.7222, -0.2391, -0.2278, -0.2432,\n",
       "         0.9551, -0.8257,  0.1303,  0.0845, -0.9726, -0.2043, -0.0751, -0.3190,\n",
       "        -0.1875, -0.0026, -0.5369,  0.1796,  0.2475, -0.2581, -0.4072,  0.3753,\n",
       "         0.2138,  0.3937, -0.0552, -0.3846, -0.4128,  0.5225,  0.1712, -0.0910,\n",
       "         0.5309,  0.8437,  0.6932, -0.6771,  0.5844, -0.1318,  0.0964,  0.3654,\n",
       "         0.4073, -0.3495, -0.5212, -0.0173, -0.0656, -0.7051, -0.0181, -0.1174,\n",
       "        -0.6284,  0.1590, -0.9738,  0.0019,  0.6969, -0.0835,  0.1399, -0.2120,\n",
       "         0.6052,  0.2179, -0.0760, -0.3116, -0.4843,  0.8462, -0.0819,  0.3877,\n",
       "         0.2572,  0.8986, -0.0341,  0.3691,  0.0783, -0.3254, -1.0125, -0.1220,\n",
       "         1.0881,  0.6419, -0.2615,  0.0723, -0.4124,  0.1466,  0.2530,  0.0599,\n",
       "         0.7032, -0.5501,  0.5050, -0.3880, -0.4303, -0.3041, -0.3142,  0.2204,\n",
       "        -0.6796, -0.0640,  0.0872, -0.2838, -0.0974, -0.1704, -0.7401,  0.4895,\n",
       "        -0.3014,  0.0279, -0.5424, -0.5825,  0.1084,  0.5348, -0.5934,  0.2757,\n",
       "        -0.4887, -0.3994,  0.1302,  0.3035, -0.1258,  0.5378,  0.2116, -0.6722,\n",
       "        -0.1801,  0.3276,  0.0853,  0.0249,  0.5532, -1.1623, -0.4999,  0.2326,\n",
       "        -0.0929, -0.3856,  0.4266, -0.4385,  0.0692, -0.2114, -0.0559,  0.0884,\n",
       "        -0.8058,  0.6601,  0.5302, -0.0788,  0.0622,  0.0390,  0.8684, -0.1012,\n",
       "         0.3403, -0.4314, -0.5968, -0.1478,  0.1891, -0.3893, -0.1543,  0.4962,\n",
       "        -1.4112,  0.5338,  1.0822,  0.9329, -0.1143,  0.4870,  0.2183, -0.2078,\n",
       "        -3.2806,  0.3317,  0.5511,  0.5228, -0.1443, -0.3328,  0.6169,  0.1898,\n",
       "         0.1859, -0.1020, -0.5276, -0.2654,  0.5447,  0.2533, -0.0365,  0.1169,\n",
       "         0.3988, -0.1499, -0.6184,  0.0143,  0.8644, -0.0941,  0.1478,  0.0038,\n",
       "        -0.3656, -0.2414, -0.0928, -0.2315,  0.1167,  0.1605,  0.5399, -0.0185,\n",
       "        -0.2937, -0.2292,  0.5265, -0.3809,  0.0476, -0.3075, -0.0293, -0.4420,\n",
       "         0.0547,  0.1005, -0.3253, -0.3749, -0.3694, -0.2338, -0.0799, -0.2595])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "italy = TEXT.vocab.vectors[TEXT.vocab.stoi['italy']];italy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator = data.BucketIterator.splits(\n",
    "    datasets = (train_data, valid_data), \n",
    "    batch_sizes = (128,64),\n",
    "    sort_key = lambda x:len(x.text),\n",
    "    sort_within_batch = True,\n",
    "    repeat = False,\n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastText(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):       \n",
    "        embedded = self.embedding(x)       \n",
    "        embedded = embedded.permute(1, 0, 2)        \n",
    "        pooled = F.avg_pool2d(embedded, (embedded.shape[1], 1)).squeeze(1)                 \n",
    "        return self.fc(pooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 200\n",
    "OUTPUT_DIM = 1\n",
    "model = FastText(INPUT_DIM, EMBEDDING_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading pretrained word embeddings as the weight initializer for our embedding layer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.5545, -0.1426, -0.0038,  ..., -0.2828, -0.0693,  1.2271],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):  \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Train loss: 0.527 | Train accuracy: 0.745 | Valid loss: 0.452 | Valid accuracy: 0.791\n",
      "Epoch: 2 | Train loss: 0.414 | Train accuracy: 0.814 | Valid loss: 0.439 | Valid accuracy: 0.796\n",
      "Epoch: 3 | Train loss: 0.378 | Train accuracy: 0.833 | Valid loss: 0.445 | Valid accuracy: 0.798\n",
      "Epoch: 4 | Train loss: 0.357 | Train accuracy: 0.844 | Valid loss: 0.458 | Valid accuracy: 0.795\n",
      "Epoch: 5 | Train loss: 0.343 | Train accuracy: 0.851 | Valid loss: 0.472 | Valid accuracy: 0.792\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    print(\"Epoch:\", epoch+1, \"| Train Loss:\", \"%.3f\" % train_loss, \"| Train Acc:\", \"%.3f\" % train_acc,\"| Valid Loss:\", \"%.3f\" % valid_loss, \"| Valid Acc:\", \"%.3f\" % valid_acc )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "def predict_sentiment(sentence):\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    prediction = torch.sigmoid(model(tensor))\n",
    "    if prediction.item()>0.5:\n",
    "        return (\"Positive\", prediction.item())\n",
    "    else:\n",
    "        return (\"Negative\", prediction.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Positive', 0.9998857975006104)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(\"this tutorial is very useful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Negative', 0.06993444263935089)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(\"i don't know why any would use tensorflow instead of pytorch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
